# Introduction to Statistics
> *"Statistics is the grammar of science."* - Karl Pearson 

## ğŸ“Š What is Statistics?
**Statistics** is the science of collecting, organizing, and analyzing data to make informed decisions and draw meaningful conclusions from information.

## ğŸ” Types of Statistics
Statistics is divided into two main branches:

### 1ï¸âƒ£ Descriptive Statistics
**Definition:** It consists of organizing and summarizing data

**Components:**
- **ğŸ“Š Measures of Central Tendency:** Mean, Median, Mode
- **ğŸ“ˆ Measures of Dispersion:** Variance, Standard Deviation
- **ğŸ“‹ Different types of Distribution of data**
  - Examples: Histogram, PDF, PMF

**Example:** Let's say there are 20 statistics classes at your college, and you have collected the heights of students in the class.

Heights recorded: `[175cm, 180cm, 140cm, 140cm, 135cm, 160cm, 185cm, 190cm]`

**Descriptive Question:** *"What is the average height of the entire classroom?"*
```
Calculation: (175+180+140+140+135+160+185+190) / 8 = Average Height
```

### 2ï¸âƒ£ Inferential Statistics  
**Definition:** It consists of using data you have measured to form conclusions

**Components:**
- **ğŸ§ª Hypothesis Testing**
  - Z-test, t-test
  - Hâ‚€, Hâ‚, p-value, significance value
- **ğŸ“Š Chi Square tests**

**Inferential Question:** *"Are the heights of the students in classroom similar to what you expect in the entire college?"*

This involves using your **sample data** (classroom) to make inferences about the **population data** (entire college).

```mermaid
graph LR
    A[ğŸ” Sample<br/>Classroom Data] --> B[ğŸ“Š Analysis] --> C[ğŸ¯ Population<br/>College Inference]
    style A fill:#fff3e0,color:#000
    style B fill:#e8f5e8,color:#000
    style C fill:#fce4ec,color:#000

```

## ğŸ¯ Population vs Sample
Understanding the relationship between population and sample is fundamental to statistics:

### ğŸŒ Population
**Definition:** The group you are interested in studying
- Represents the **entire** group of interest
- Usually large and difficult to study completely
- Example: All students in the college

### ğŸ“‹ Sample  
**Definition:** A subset of population
- A **smaller, manageable** portion of the population
- Used to make inferences about the population
- Example: Students in one statistics classroom

```mermaid
graph TD
    A[ğŸŒ Population<br/>All College Students] --> B[ğŸ“‹ Sample<br/>One Classroom]
    B --> C[ğŸ“Š Statistical Analysis]
    C --> D[ğŸ” Inferences about Population]

    style A fill:#ffebee,color:#000
    style B fill:#fff3e0,color:#000
    style C fill:#e8f5e8,color:#000
    style D fill:#fce4ec,color:#000
```

### ğŸ”„ The Statistical Inference Process
```mermaid
flowchart LR
    A[ğŸ¯ Population<br/>Parameter] -.->|"We want to know"| B[â“ Unknown<br/>Population Truth]
    C[ğŸ“‹ Sample<br/>Statistic] -->|"We calculate"| D[ğŸ“Š Sample Result]
    D -->|"We infer"| B

    style A fill:#ffcdd2,color:#000
    style C fill:#fff3e0,color:#000
    style D fill:#e8f5e8,color:#000
    style B fill:#fce4ec,color:#000
```

**Key Point:** We use sample statistics to estimate population parameters!

## ğŸ² Random Variables

A **Random Variable** is a process of mapping the output of a random process or experiments to a number.

**Definition:** A random variable assigns numerical values to the outcomes of a random experiment or process.

### ğŸ“ Examples of Random Variables

#### ğŸª™ Coin Tossing
```
X = { 0  if Head
    { 1  if Tails
```

#### ğŸ² Rolling a Dice
The outcome when rolling a standard six-sided dice: X can take values {1, 2, 3, 4, 5, 6}

#### ğŸŒ¡ï¸ Temperature Measurement
Measuring the temperature of the next day - X represents the temperature value

#### ğŸ¯ Complex Random Variables
**Y = Sum of rolling a dice 7 times**

This can be used to calculate probabilities:
- P(Y > 15) - Probability that sum is greater than 15
- P(Y < 10) - Probability that sum is less than 10

### ğŸ”¢ Types of Random Variables

Random variables can be classified as:
- **Discrete Random Variables:** Can only take specific, countable values (like dice outcomes: 1, 2, 3, 4, 5, 6)
- **Continuous Random Variables:** Can take any value within a range (like temperature: 25.3Â°C, 25.31Â°C, etc.)

## ğŸ“š Set Theory Fundamentals

Understanding set theory is crucial for probability and statistics. Here are the key set operations:

### ğŸ” Basic Set Concepts

#### ğŸ“Š Sets
**Set A:** {1, 2, 3, 4, 5, 6, 7, 8}
**Set B:** {3, 4, 5, 6, 7}

### ğŸ”„ Set Operations

#### 1ï¸âƒ£ Intersection (A âˆ© B)
**Definition:** Elements that are common to both sets
**Result:** A âˆ© B = {3, 4, 5, 6, 7}

**Visual Representation:** The overlapping region in a Venn diagram
```
    A     B
   â•­â”€â”€â”€â•® â•­â”€â”€â”€â•®
  â•±     â•²â•±     â•²
 â•±   A   â•³  B   â•²
â•±        â•±â•²      â•²
â•²       â•±  â•²     â•±
 â•²     â•± âˆ©  â•²   â•±
  â•²___â•±      â•²_â•±
```

#### 2ï¸âƒ£ Union (A âˆª B)
**Definition:** All elements that belong to either set A or set B (or both)
**Result:** A âˆª B = {1, 2, 3, 4, 5, 6, 7, 8}

**Visual Representation:** The entire shaded area in both circles
```
    A     B
   â•­â•â•â•â•® â•­â•â•â•â•®
  â•‘     â•²â•±     â•‘
 â•‘   A   â•³  B   â•‘
â•‘        â•±â•²      â•‘
â•šâ•â•â•â•â•â•â•â•±  â•²â•â•â•â•â•â•
       â•± âˆª  â•²   
      â•±      â•² 
```

#### 3ï¸âƒ£ Difference (A - B)
**Definition:** Elements that are in set A but not in set B
**Result:** A - B = {1, 2, 8}

**Visual Representation:** The part of A that doesn't overlap with B
```
    A     B
   â•­â•â•â•â•® â•­â”€â”€â”€â•®
  â•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•²â•±     â•²
 â•‘â–ˆâ–ˆâ–ˆAâ–ˆâ–ˆâ–ˆâ•³  B   â•²
â•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•±â•²      â•²
â•šâ•â•â•â•â•â•â•â•±  â•²     â•±
       â•±    â•²   â•±
      â•±      â•²_â•±
```

### ğŸ”— Set Relationships

#### 4ï¸âƒ£ Subset
**A â†’ B:** "A is a subset of B" means every element of A is also in B
- **Example:** A â†’ B = False (because 1, 2, 8 are in A but not in B)

#### 5ï¸âƒ£ Superset  
**A â†’ B:** "A is a superset of B" means A contains all elements of B
- **Example:** B â†’ A = True (because all elements of B {3,4,5,6,7} are in A)

### ğŸ¯ Set Theory Applications in Probability

Set theory forms the foundation for probability concepts:
- **Sample Space (S):** The set of all possible outcomes
- **Events:** Subsets of the sample space
- **Intersection:** Joint events (A AND B)
- **Union:** Combined events (A OR B)
- **Complement:** Events that don't occur

## ğŸ“Š Measures of Central Tendency

Central tendency describes where the center of a dataset lies. There are three primary measures:

### 1ï¸âƒ£ Mean (Average)
**Definition:** The arithmetic average of all values in a dataset

#### Population Mean (Î¼)
For a population with N values:
```
Population Mean (Î¼) = Î£ Xi / N
                    = (Xâ‚ + Xâ‚‚ + Xâ‚ƒ + ... + Xâ‚™) / N
```

#### Sample Mean (xÌ„)
For a sample with n values:
```
Sample Mean (xÌ„) = Î£ Xi / n
                 = (Xâ‚ + Xâ‚‚ + Xâ‚ƒ + ... + Xâ‚™) / n
```

**Example:**
Population: X = {1, 1, 2, 2, 3, 3, 4, 5, 5, 6}
```
Population Mean (Î¼) = (1+1+2+2+3+3+4+5+5+6) / 10 = 32 / 10 = 3.2
```

**Programming Implementation:**
```python
import numpy as np

# Calculate mean using numpy
age = [12, 21, 23, 45, 65, 43, 56, 45, 32, 67, 54, 34]
mean_age = np.mean(age)
print(f"Mean age: {mean_age}")  # Output: 41.41666666666667
```

### 2ï¸âƒ£ Median
**Definition:** The middle value when data is arranged in ascending or descending order

#### Steps to Find Median:
1. **Sort the data** in ascending order
2. **Count the number of elements**
3. **If count is odd:** Median = middle value
4. **If count is even:** Median = average of two middle values

**Examples:**

**Case 1: Odd number of elements**
```
X = {4, 5, 2, 3, 2, 1}
Sorted: {1, 2, 2, 3, 4, 5}
Count = 6 (even)
Median = (2 + 3) / 2 = 2.5
```

**Case 2: Even number of elements**  
```
X = {1, 2, 3, 4, 5, 100}
Count = 6 (even)
Middle positions: 3rd and 4th values = 3 and 4
Median = (3 + 4) / 2 = 3.5
```

**Why Use Median?**
Median is useful when **outliers are present** in the data because it's less affected by extreme values.

**Comparison Example:**
- Without outlier: X = {1, 2, 3, 4, 5} â†’ Mean = 3, Median = 3
- With outlier: X = {1, 2, 3, 4, 5, 100} â†’ Mean â‰ˆ 19.2, Median = 3.5

The median (3.5) better represents the central tendency when outliers are present.

**Programming Implementation:**
```python
import numpy as np

# Calculate median using numpy
age = [12, 21, 23, 45, 65, 43, 56, 45, 32, 67, 54, 34, 200]
median_age = np.median(age)
print(f"Median age: {median_age}")  # Output: 45.0
```

### 3ï¸âƒ£ Mode
**Definition:** The value(s) that appear most frequently in a dataset

**Characteristics:**
- **Most frequent value** in the dataset
- A dataset can have:
  - **No mode** (all values appear equally)
  - **One mode** (unimodal)
  - **Two modes** (bimodal)  
  - **Multiple modes** (multimodal)

**Example:**
```
Dataset: {2, 1, 1, 1, 4, 5, 7, 8, 9, 9, 10}

Frequency count:
1 appears 3 times
9 appears 2 times  
All others appear 1 time each

Mode = 1 (highest frequency)
```

**Programming Implementation:**
```python
from scipy import stats

# Calculate mode using scipy
age = [12, 21, 23, 45, 65, 43, 56, 45, 32, 67, 54, 34, 200]
mode_result = stats.mode(age)
print(f"Mode: {mode_result}")  # Returns mode value and count
```

### ğŸ“Š Central Tendency Comparison

| **Measure** | **Best Used When** | **Advantages** | **Disadvantages** | **Affected by Outliers?** |
|-------------|-------------------|----------------|-------------------|---------------------------|
| **Mean** | Normal distribution, no outliers | Uses all data points | Sensitive to outliers | Yes |
| **Median** | Skewed data, outliers present | Resistant to outliers | Ignores extreme values | No |
| **Mode** | Categorical data, finding most common | Shows most frequent value | May not exist or be unique | No |

### ğŸ¯ When to Use Each Measure

```mermaid
flowchart TD
    A[ğŸ“Š Choose Central Tendency] --> B{Data Type?}
    
    B -->|Numerical| C{Distribution Shape?}
    B -->|Categorical| D[ğŸ“ Use Mode]
    
    C -->|Normal/Symmetric| E[ğŸ“Š Use Mean]
    C -->|Skewed| F[ğŸ“Š Use Median]
    C -->|Has Outliers| G[ğŸ“Š Use Median]
    
    style A fill:#e3f2fd,color:#000
    style D fill:#fff3e0,color:#000
    style E fill:#e8f5e8,color:#000
    style F fill:#fce4ec,color:#000
    style G fill:#fce4ec,color:#000
```

### ğŸ“ˆ Practical Example: Complete Analysis

Let's analyze a complete dataset:

**Dataset:** Ages of students in a class
```python
ages = [18, 19, 20, 20, 21, 21, 21, 22, 23, 45]
```

**Calculations:**
```python
import numpy as np
from scipy import stats

# Mean
mean_age = np.mean(ages)
print(f"Mean: {mean_age}")  # 23.0

# Median  
median_age = np.median(ages)
print(f"Median: {median_age}")  # 21.0

# Mode
mode_age = stats.mode(ages)
print(f"Mode: {mode_age[0]}")  # 21
```

**Analysis:**
- **Mean (23.0):** Pulled up by the outlier (45)
- **Median (21.0):** Better represents the typical student age
- **Mode (21):** Most common age in the class

In this case, **median** gives the best representation of central tendency due to the outlier.

## ğŸ“ Measures of Dispersion (Spread of Data)

While measures of central tendency tell us about the center of our data, **measures of dispersion** tell us how spread out or scattered the data points are from the center. Understanding dispersion is crucial because two datasets can have the same mean but very different spreads.

### ğŸ“Š What is Dispersion?
**Dispersion** refers to how much the data points deviate from the central value (mean). It helps us understand:
- How consistent or variable our data is
- The reliability of our central tendency measures
- The range of values we can expect in our dataset

### ğŸ“ˆ Visual Understanding of Dispersion

```mermaid
graph LR
    A[Low Dispersion<br/>SÂ² = 2.5] --> B[ğŸ“Š Data clustered<br/>around center]
    C[High Dispersion<br/>SÂ² = 9.5] --> D[ğŸ“Š Data spread out<br/>from center]
    
    style A fill:#e8f5e8,color:#000
    style B fill:#e8f5e8,color:#000
    style C fill:#ffebee,color:#000
    style D fill:#ffebee,color:#000
```

Two datasets can have the same mean but different spreads:
- **Dataset A:** {49, 50, 51} â†’ Mean = 50, Low dispersion
- **Dataset B:** {30, 50, 70} â†’ Mean = 50, High dispersion

### 1ï¸âƒ£ Variance

**Definition:** Variance measures the average of squared differences from the mean. It tells us how much the data points deviate from the mean on average.

#### Population Variance (ÏƒÂ²)
For a population with N values:
```
Population Variance (ÏƒÂ²) = Î£(Xi - Î¼)Â² / N

Where:
â€¢ Xi = Individual data points
â€¢ Î¼ = Population mean  
â€¢ N = Population size
```

#### Sample Variance (SÂ²)
For a sample with n values:
```
Sample Variance (SÂ²) = Î£(Xi - xÌ„)Â² / (n-1)

Where:
â€¢ Xi = Individual data points
â€¢ xÌ„ = Sample mean
â€¢ n = Sample size
```

#### ğŸ¤” Why do we divide Sample Variance by (n-1)?
This is called **Bessel's Correction** and it helps us create an **unbiased estimator** of the population variance. When we use a sample to estimate population parameters, dividing by (n-1) instead of n gives us a more accurate estimate.

### 2ï¸âƒ£ Standard Deviation

**Definition:** Standard deviation is the square root of variance. It's expressed in the same units as the original data, making it easier to interpret.

#### Population Standard Deviation (Ïƒ)
```
Population Standard Deviation (Ïƒ) = âˆš(Variance) = âˆšÏƒÂ²
```

#### Sample Standard Deviation (S)
```
Sample Standard Deviation (S) = âˆš(Sample Variance) = âˆšSÂ²
```

### ğŸ“Š Sample Variance Practical Example

Let's work through the example shown in the lecture:

**Dataset:** X = {1, 2, 3, 4, 5}

#### Step-by-Step Calculation:

**Step 1:** Calculate the mean (xÌ„)
```
xÌ„ = (1 + 2 + 3 + 4 + 5) / 5 = 15 / 5 = 3
```

**Step 2:** Calculate deviations from mean and square them
```
(1-3)Â² = (-2)Â² = 4
(2-3)Â² = (-1)Â² = 1  
(3-3)Â² = (0)Â² = 0
(4-3)Â² = (1)Â² = 1
(5-3)Â² = (2)Â² = 4

Sum of squared deviations: 4 + 1 + 0 + 1 + 4 = 10
```

**Step 3:** Apply Sample Variance Formula
```
Sample Variance (SÂ²) = Î£(Xi - xÌ„)Â² / (n-1)
                     = 10 / (5-1)
                     = 10 / 4
                     = 2.5
```

**Step 4:** Calculate Sample Standard Deviation
```
Sample Standard Deviation (S) = âˆšSÂ² = âˆš2.5 = 1.58
```

### ğŸ“Š Normal Distribution and Standard Deviation

Understanding how data spreads in a normal distribution is crucial for interpreting standard deviation:

#### ğŸ“ˆ The 68-95-99.7 Rule (Empirical Rule)

In a normal distribution:
- **68%** of data falls within **1 standard deviation** of the mean
- **95%** of data falls within **2 standard deviations** of the mean  
- **99.7%** of data falls within **3 standard deviations** of the mean

**Visual Representation:**
```
       Î¼-3Ïƒ    Î¼-2Ïƒ    Î¼-1Ïƒ     Î¼      Î¼+1Ïƒ    Î¼+2Ïƒ    Î¼+3Ïƒ
        |       |       |       |       |       |       |
    0.1%|  2.1% | 13.6% | 34.1% | 34.1% | 13.6% |  2.1% |0.1%
        |       |       |       |       |       |       |
        |-------|-------|-------|-------|-------|-------|
              68% of data (Â±1Ïƒ)
                    95% of data (Â±2Ïƒ)  
                          99.7% of data (Â±3Ïƒ)
```

#### Example with Normal Distribution:
If test scores follow a normal distribution with:
- **Mean (Î¼) = 75**  
- **Standard Deviation (Ïƒ) = 10**

Then:
- **68%** of students scored between **65-85** (75 Â± 10)
- **95%** of students scored between **55-95** (75 Â± 20)
- **99.7%** of students scored between **45-105** (75 Â± 30)

### ğŸ“Š Practical Programming Example

```python
import numpy as np
import pandas as pd

# Dataset from the lecture example
ages = [23, 43, 23, 56, 74, 32, 68, 98, 45, 32]

# Calculate mean
mean_age = np.mean(ages)
print(f"Mean: {mean_age}")  # Output: 49.4

# Calculate Sample Variance (using n-1)
sample_variance = np.var(ages, ddof=1)  # ddof=1 uses n-1
print(f"Sample Variance: {sample_variance}")  # Output: ~601.82

# Calculate Population Variance (using n)
population_variance = np.var(ages, ddof=0)  # ddof=0 uses n
print(f"Population Variance: {population_variance}")  # Output: ~541.64

# Calculate Standard Deviations
sample_std = np.std(ages, ddof=1)
population_std = np.std(ages, ddof=0)

print(f"Sample Std Dev: {sample_std}")      # Output: ~24.53
print(f"Population Std Dev: {population_std}")  # Output: ~23.27
```

### ğŸ“Š Working with DataFrames - Variance by Rows and Columns

From the lecture, we can see how to calculate variance for different axes:

```python
import pandas as pd
import numpy as np

# Create sample DataFrame (from lecture)
data = [[10, 12, 13], 
        [34, 23, 45], 
        [32, 34, 21]]
df = pd.DataFrame(data, columns=["A", "B", "C"])

print("DataFrame:")
print(df)
#     A   B   C
# 0  10  12  13
# 1  34  23  45
# 2  32  34  21

# Calculate variance for each COLUMN (axis=0, default)
variance_by_column = df.var()
print(f"\nVariance by Column:")
print(variance_by_column)
# A    177.333333
# B    121.000000
# C    277.333333

# Calculate variance for each ROW (axis=1)  
variance_by_row = df.var(axis=1)
print(f"\nVariance by Row:")
print(variance_by_row)
# 0      2.333333
# 1    126.333333  
# 2     42.333333

# Calculate standard deviation
std_by_column = df.std()
print(f"\nStandard Deviation by Column:")
print(std_by_column)
# A    13.316624
# B    11.000000
# C    16.653332
```

**Interpretation:**
- **Column A variance (177.33):** High spread in column A values (10, 34, 32)
- **Column B variance (121.00):** Moderate spread in column B values (12, 23, 34)
- **Column C variance (277.33):** Highest spread in column C values (13, 45, 21)
- **Row variances:** Show how much values vary within each row

### ğŸ¯ Interpreting Variance and Standard Deviation

| **Value** | **Interpretation** | **Example** |
|-----------|-------------------|-------------|
| **Low Variance/Std Dev** | Data points are close to the mean | Test scores: 85, 87, 86, 88, 84 |
| **High Variance/Std Dev** | Data points are spread out from the mean | Test scores: 60, 95, 70, 40, 85 |
| **Zero Variance** | All data points are identical | Test scores: 80, 80, 80, 80, 80 |

### ğŸ” Key Differences: Population vs Sample

| **Aspect** | **Population** | **Sample** |
|------------|----------------|------------|
| **Variance Formula** | ÏƒÂ² = Î£(Xi-Î¼)Â²/N | SÂ² = Î£(Xi-xÌ„)Â²/(n-1) |
| **Std Dev Formula** | Ïƒ = âˆšÏƒÂ² | S = âˆšSÂ² |
| **Denominator** | N (population size) | n-1 (degrees of freedom) |
| **Purpose** | Describes entire population | Estimates population parameters |
| **Symbol** | ÏƒÂ² (sigma squared), Ïƒ | SÂ², S |

### ğŸ’¡ Practical Applications

**1. Quality Control:**
- Manufacturing: Monitor product consistency
- Low variance = consistent quality
- High variance = quality issues

**2. Finance:**
- Investment risk assessment
- Low variance = stable returns  
- High variance = volatile returns

**3. Education:**
- Student performance analysis
- Low variance = consistent class performance
- High variance = mixed ability levels

**4. Healthcare:**
- Treatment effectiveness
- Low variance = predictable treatment outcomes
- High variance = unpredictable results

### ğŸ“š Summary: Measures of Dispersion

```
ğŸ“ Measures of Dispersion = How spread out the data is

ğŸ”¢ Variance = Average of squared differences from mean
   â€¢ Population: ÏƒÂ² = Î£(Xi-Î¼)Â²/N  
   â€¢ Sample: SÂ² = Î£(Xi-xÌ„)Â²/(n-1)

ğŸ“ Standard Deviation = Square root of variance
   â€¢ Population: Ïƒ = âˆšÏƒÂ²
   â€¢ Sample: S = âˆšSÂ²

ğŸ¯ Why Standard Deviation?
   â€¢ Same units as original data
   â€¢ Easier to interpret than variance
   â€¢ Shows typical deviation from mean

ğŸ”„ Bessel's Correction (n-1):
   â€¢ Used in sample calculations
   â€¢ Provides unbiased estimate
   â€¢ Accounts for sampling variability

ğŸ“Š Interpretation:
   â€¢ Low values = Data clustered near mean
   â€¢ High values = Data spread out from mean  
   â€¢ Zero = All data points identical
```

## ğŸ“Š Histograms and Data Distribution

Histograms are one of the most fundamental tools in statistics for visualizing the distribution of data. They help us understand the shape, center, and spread of our dataset at a glance.

### ğŸ“Š What is a Histogram?

**Definition:** A histogram is a graphical representation that shows the distribution of numerical data by dividing the data into bins (intervals) and displaying the frequency of data points in each bin.

**Key Components:**
- **X-axis:** Data values divided into bins (intervals)
- **Y-axis:** Frequency (count) of data points in each bin
- **Bars:** Height represents frequency, width represents bin size

### ğŸ”¢ Creating Histograms: Bin Size Calculation

From the lecture example with age data:
```
Ages = {10, 12, 19, 18, 24, 26, 30, 35, 36, 37, 42, 41, 42, 45, 50, 51}
```

#### ğŸ“ Determining Bin Size
There are different methods to determine optimal bin size:

**Method 1: Simple Division**
```
Bin Size = (Max - Min) / Number of desired bins
= (51 - 10) / 10 = 4.1 â‰ˆ 5
```

**Method 2: Sturges' Rule**
```
Number of bins = âŒˆlogâ‚‚(n) + 1âŒ‰
Where n = number of data points
```

**Method 3: Square Root Rule**
```
Number of bins = âŒˆâˆšnâŒ‰
```

#### ğŸ“Š Practical Example from Lecture
For the age dataset:
- **Bin Size = 5** â†’ Number of bins = 10
- **Bin Size = 2.5** â†’ Number of bins = 20

### ğŸ“ˆ From Histogram to Probability Distribution

#### ğŸ”„ Converting Frequency to Probability
When we have a histogram showing frequencies, we can convert it to a **probability distribution**:

```
P(x) = Frequency of bin / Total number of observations
```

**Example:**
- If bin [20-25) has frequency = 4
- Total observations = 16
- Then P(20 â‰¤ x < 25) = 4/16 = 0.25 = 25%

#### ğŸ“Š Probability Density Function (PDF)
- **For continuous data:** The histogram approaches a smooth curve (PDF) as sample size increases and bin width decreases
- **Area under the curve = 1** (total probability)
- **Height = Probability density** (not probability itself)

### ğŸ” Distribution Shapes and Skewness

Understanding the shape of your data distribution is crucial for selecting appropriate statistical methods and interpreting results.

### ğŸ“Š Types of Distribution Shapes

#### 1ï¸âƒ£ **Symmetric Distribution (Normal/Gaussian)**

**Characteristics:**
- **Perfect symmetry** around the center
- **Mean = Median = Mode** (all at the center)
- **Bell-shaped curve** when data is normally distributed
- **Equal spread** on both sides of the center

```
    Frequency
        â†‘
        |     â•­â”€â•®
        |   â•­â”€â•¯ â•°â”€â•®
        |  â•±       â•²
        | â•±         â•²
        |â•±___________â•²
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Values
              Î¼
```

**Box Plot Characteristics:**
- Qâ‚‚ (median) is **exactly in the middle** of Qâ‚ and Qâ‚ƒ
- **Qâ‚ƒ - Qâ‚‚ â‰ˆ Qâ‚‚ - Qâ‚** (equal distances)

**Real-World Examples:**
- Heights of people
- IQ scores
- Measurement errors
- Blood pressure readings

#### 2ï¸âƒ£ **Right Skewed Distribution (Positive Skew)**

**Characteristics:**
- **Long tail extends to the right**
- **Mean > Median > Mode** 
- **Majority of data** concentrated on the left side
- **Few extreme values** pull the mean to the right

```
    Frequency
        â†‘
        |â•­â”€â•®
        |â–ˆ â•°â”€â•®
        |â–ˆ   â•°â”€â•®
        |â–ˆ     â•°â”€â•®
        |â–ˆ_______â•°â”€â”€â”€â”€â”€â•®
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Values
         â†‘  â†‘     â†‘
       Mode Med  Mean
```

**Box Plot Characteristics:**
- **Qâ‚ƒ - Qâ‚‚ > Qâ‚‚ - Qâ‚** (right whisker longer)
- **Median closer to Qâ‚** than to Qâ‚ƒ
- **Potential outliers** on the right side

**Real-World Examples:**
- Income distribution (few very wealthy people)
- House prices in a city
- Response times (most fast, few very slow)
- Age at death (most people live to old age)

#### 3ï¸âƒ£ **Left Skewed Distribution (Negative Skew)**

**Characteristics:**
- **Long tail extends to the left**
- **Mean < Median < Mode**
- **Majority of data** concentrated on the right side
- **Few extreme values** pull the mean to the left

```
    Frequency
        â†‘
        |        â•­â”€â•®â–ˆ
        |      â•­â”€â•¯ â–ˆ
        |    â•­â”€â•¯   â–ˆ
        |  â•­â”€â•¯     â–ˆ
        |â•­â”€_______â–ˆ
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Values
         â†‘     â†‘  â†‘
       Mean   Med Mode
```

**Box Plot Characteristics:**
- **Qâ‚‚ - Qâ‚ > Qâ‚ƒ - Qâ‚‚** (left whisker longer)
- **Median closer to Qâ‚ƒ** than to Qâ‚
- **Potential outliers** on the left side

**Real-World Examples:**
- Test scores (most students do well, few fail)
- Age at retirement (most retire at standard age, few retire very early)
- Customer satisfaction ratings (most satisfied, few very unsatisfied)

### ğŸ“Š Skewness and Central Tendency Relationships

#### ğŸ“ˆ Summary of Relationships

| **Distribution Type** | **Shape** | **Central Tendency Relationship** | **Tail Direction** |
|----------------------|-----------|-------------------------------------|-------------------|
| **Symmetric (Normal)** | Bell-shaped | **Mean = Median = Mode** | No tail |
| **Right Skewed (Positive)** | Long right tail | **Mean > Median > Mode** | Right tail |
| **Left Skewed (Negative)** | Long left tail | **Mean < Median < Mode** | Left tail |

#### ğŸ¯ Key Insight: 
**The mean always gets "pulled" toward the tail direction due to extreme values, while the median remains more stable.**

### ğŸ“¦ Box Plots and Distribution Analysis

Box plots (also called box-and-whisker plots) provide a visual summary of data distribution and help identify skewness.

#### ğŸ“Š Box Plot Components

```
    Qâ‚        Qâ‚‚(Median)    Qâ‚ƒ
     |            |         |
  â”Œâ”€â”€â”´â”€â”€â”    â”Œâ”€â”€â”€â”´â”€â”€â”€â”    â”Œâ”´â”€â”€â”
  â”‚     â”‚    â”‚       â”‚    â”‚   â”‚
â”€â”€â”¤     â”œâ”€â”€â”€â”€â”¤   â–ˆ   â”œâ”€â”€â”€â”€â”¤   â”œâ”€â”€
  â”‚     â”‚    â”‚       â”‚    â”‚   â”‚
  â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”˜
     â†‘            â†‘         â†‘
   Min         Median     Max
(or whisker)              (or whisker)
```

**Components:**
- **Qâ‚ (First Quartile):** 25% of data falls below this value
- **Qâ‚‚ (Second Quartile/Median):** 50% of data falls below this value  
- **Qâ‚ƒ (Third Quartile):** 75% of data falls below this value
- **IQR (Interquartile Range):** Qâ‚ƒ - Qâ‚
- **Whiskers:** Extend to min/max or 1.5Ã—IQR from quartiles
- **Outliers:** Points beyond the whiskers

#### ğŸ” Identifying Skewness from Box Plots

**Symmetric Distribution:**
- **Median line** in the **center** of the box
- **Equal whisker lengths:** Qâ‚ƒ - Qâ‚‚ â‰ˆ Qâ‚‚ - Qâ‚
- **Few or no outliers**

**Right Skewed Distribution:**
- **Median line** closer to **Qâ‚** (left side of box)
- **Longer right whisker:** Qâ‚ƒ - Qâ‚‚ > Qâ‚‚ - Qâ‚
- **Outliers** more likely on the **right side**

**Left Skewed Distribution:**
- **Median line** closer to **Qâ‚ƒ** (right side of box)
- **Longer left whisker:** Qâ‚‚ - Qâ‚ > Qâ‚ƒ - Qâ‚‚
- **Outliers** more likely on the **left side**

### ğŸ“Š Practical Applications and Decision Making

Understanding distribution shapes helps in:

#### 1ï¸âƒ£ **Choosing Appropriate Statistics**
- **Symmetric data:** Use mean and standard deviation
- **Skewed data:** Use median and IQR (more robust)

#### 2ï¸âƒ£ **Data Transformation**
- **Right skewed:** Apply log transformation
- **Left skewed:** Apply square or exponential transformation

#### 3ï¸âƒ£ **Outlier Detection**
- **Box plots** help identify potential outliers
- **Outliers** may indicate data errors or special cases

#### 4ï¸âƒ£ **Statistical Test Selection**
- **Normal distribution:** Use parametric tests (t-test, ANOVA)
- **Skewed distribution:** Use non-parametric tests (Mann-Whitney, Kruskal-Wallis)

### ğŸ’» Programming Implementation

#### Creating Histograms and Analyzing Distribution:

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Sample age data from lecture
ages = [10, 12, 19, 18, 24, 26, 30, 35, 36, 37, 42, 41, 42, 45, 50, 51]

# Create histogram
plt.figure(figsize=(12, 8))

# Subplot 1: Histogram with different bin sizes
plt.subplot(2, 3, 1)
plt.hist(ages, bins=5, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('Histogram - 5 bins')
plt.xlabel('Age')
plt.ylabel('Frequency')

plt.subplot(2, 3, 2)
plt.hist(ages, bins=10, alpha=0.7, color='lightgreen', edgecolor='black')
plt.title('Histogram - 10 bins')
plt.xlabel('Age')
plt.ylabel('Frequency')

# Subplot 3: Box plot
plt.subplot(2, 3, 3)
plt.boxplot(ages, vert=True)
plt.title('Box Plot')
plt.ylabel('Age')

# Calculate statistics
mean_age = np.mean(ages)
median_age = np.median(ages)
mode_result = stats.mode(ages)
std_age = np.std(ages)
skewness = stats.skew(ages)

# Subplot 4: Statistics summary
plt.subplot(2, 3, 4)
plt.text(0.1, 0.8, f'Mean: {mean_age:.2f}', fontsize=12, transform=plt.gca().transAxes)
plt.text(0.1, 0.7, f'Median: {median_age:.2f}', fontsize=12, transform=plt.gca().transAxes)
plt.text(0.1, 0.6, f'Mode: {mode_result.mode[0]}', fontsize=12, transform=plt.gca().transAxes)
plt.text(0.1, 0.5, f'Std Dev: {std_age:.2f}', fontsize=12, transform=plt.gca().transAxes)
plt.text(0.1, 0.4, f'Skewness: {skewness:.2f}', fontsize=12, transform=plt.gca().transAxes)
plt.text(0.1, 0.2, 'Interpretation:', fontsize=12, weight='bold', transform=plt.gca().transAxes)
if skewness > 0.5:
    plt.text(0.1, 0.1, 'Right Skewed', fontsize=12, color='red', transform=plt.gca().transAxes)
elif skewness < -0.5:
    plt.text(0.1, 0.1, 'Left Skewed', fontsize=12, color='blue', transform=plt.gca().transAxes)
else:
    plt.text(0.1, 0.1, 'Approximately Symmetric', fontsize=12, color='green', transform=plt.gca().transAxes)
plt.axis('off')
plt.title('Statistical Summary')

plt.tight_layout()
plt.show()
```

#### Creating Different Distribution Shapes:

```python
# Generate different distribution types
np.random.seed(42)

# Normal distribution
normal_data = np.random.normal(50, 10, 1000)

# Right skewed distribution  
right_skewed = np.random.exponential(2, 1000)

# Left skewed distribution
left_skewed = 10 - np.random.exponential(2, 1000)

# Create comparative plots
fig, axes = plt.subplots(3, 2, figsize=(15, 12))

# Normal distribution
axes[0, 0].hist(normal_data, bins=30, alpha=0.7, color='skyblue', density=True)
axes[0, 0].set_title('Normal Distribution\nMean â‰ˆ Median â‰ˆ Mode')
axes[0, 0].axvline(np.mean(normal_data), color='red', linestyle='--', label=f'Mean: {np.mean(normal_data):.1f}')
axes[0, 0].axvline(np.median(normal_data), color='green', linestyle='--', label=f'Median: {np.median(normal_data):.1f}')
axes[0, 0].legend()

axes[0, 1].boxplot(normal_data, vert=True)
axes[0, 1].set_title('Box Plot - Symmetric')

# Right skewed distribution
axes[1, 0].hist(right_skewed, bins=30, alpha=0.7, color='orange', density=True)
axes[1, 0].set_title('Right Skewed Distribution\nMean > Median > Mode')
axes[1, 0].axvline(np.mean(right_skewed), color='red', linestyle='--', label=f'Mean: {np.mean(right_skewed):.1f}')
axes[1, 0].axvline(np.median(right_skewed), color='green', linestyle='--', label=f'Median: {np.median(right_skewed):.1f}')
axes[1, 0].legend()

axes[1, 1].boxplot(right_skewed, vert=True)
axes[1, 1].set_title('Box Plot - Right Skewed')

# Left skewed distribution
axes[2, 0].hist(left_skewed, bins=30, alpha=0.7, color='lightcoral', density=True)
axes[2, 0].set_title('Left Skewed Distribution\nMean < Median < Mode')
axes[2, 0].axvline(np.mean(left_skewed), color='red', linestyle='--', label=f'Mean: {np.mean(left_skewed):.1f}')
axes[2, 0].axvline(np.median(left_skewed), color='green', linestyle='--', label=f'Median: {np.median(left_skewed):.1f}')
axes[2, 0].legend()

axes[2, 1].boxplot(left_skewed, vert=True)
axes[2, 1].set_title('Box Plot - Left Skewed')

plt.tight_layout()
plt.show()
```

### ğŸ¯ Key Takeaways for Histogram and Distribution Analysis

```
ğŸ“Š Histograms show the shape and distribution of data
ğŸ“ Bin size affects the appearance - experiment with different sizes
ğŸ“ˆ Distribution shape determines appropriate statistical methods:

ğŸ”” Normal/Symmetric Distribution:
   â€¢ Mean = Median = Mode
   â€¢ Use mean and standard deviation
   â€¢ Apply parametric tests

ğŸ“Š Right Skewed (Positive Skew):
   â€¢ Mean > Median > Mode  
   â€¢ Use median and IQR
   â€¢ Consider log transformation
   â€¢ Apply non-parametric tests

ğŸ“Š Left Skewed (Negative Skew):
   â€¢ Mean < Median < Mode
   â€¢ Use median and IQR  
   â€¢ Consider power transformation
   â€¢ Apply non-parametric tests

ğŸ“¦ Box plots help identify:
   â€¢ Distribution shape and skewness
   â€¢ Outliers and extreme values
   â€¢ Quartile positions and spread

ğŸ¯ Always visualize your data first before choosing statistical methods!
```

## ğŸ“‹ Types of Data
Data can be broadly categorized into two main types:

```mermaid
graph TD
    A[ğŸ“Š DATA] --> B[ğŸ”¢ Quantitative<br/>Numerical]
    A --> C[ğŸ“ Qualitative<br/>Categorical]

    B --> D[ğŸ¯ Discrete<br/>Whole Numbers]
    B --> E[ğŸ“ˆ Continuous<br/>Any Value]

    C --> F[ğŸ·ï¸ Nominal<br/>No Order]
    C --> G[ğŸ“Š Ordinal<br/>Has Rank]

    style A fill:#e3f2fd,color:#000
    style B fill:#fff3e0,color:#000
    style C fill:#fce4ec,color:#000
    style D fill:#e8f5e8,color:#000
    style E fill:#e8f5e8,color:#000
    style F fill:#f3e5f5,color:#000
    style G fill:#f3e5f5,color:#000
```

### ğŸ”¢ Quantitative Data (Numerical)
**Definition:** Data that represents numbers and amounts, can perform mathematical operations (+, -, %, *)

#### ğŸ¯ Discrete Data
- **Definition:** Whole numbers, countable values
- **Characteristics:** Cannot be broken down into smaller meaningful units
- **Examples:**
  - Number of bank accounts: 1, 2, 3, 5
  - Number of children in a family: 0, 1, 2, 4
  - Number of students in a class: 25, 30, 45

#### ğŸ“ˆ Continuous Data  
- **Definition:** Can take any numerical value within a range
- **Characteristics:** Can be measured with infinite precision
- **Examples:**
  - Weight: 65.5 kg, 72.3 kg, 80.125 kg
  - Height: 165.2 cm, 175.8 cm, 180.25 cm
  - Temperature: 25.7C, 32.4C, 18.9C
  - Speed: 60.5 km/h, 85.3 km/h

### ğŸ“ Qualitative Data (Categorical)
**Definition:** Data that represents categories, qualities, or characteristics

#### ğŸ·ï¸ Nominal Data
- **Definition:** Categories with no intrinsic order or ranking
- **Characteristics:** Just labels or names, no mathematical operations possible
- **Examples:**
  - Gender: Male (M), Female (F)
  - Blood Group: A, B, AB, O
  - Pincode: 110001, 400001, 600001
  - Colors: Red, Blue, Green, Yellow

#### ğŸ“Š Ordinal Data
- **Definition:** Categories with a natural order or ranking
- **Characteristics:** Has meaningful sequence but intervals aren't necessarily equal
- **Examples:**
  - Customer Feedback: Good, Bad, Better, Best
  - Education Level: High School, Bachelor's, Master's, PhD
  - Star Ratings: â­, â­â­, â­â­â­, â­â­â­â­, â­â­â­â­â­

### ğŸ“Š Data Types Summary Table

| **Data Type** | **Subtype** | **Description** | **Examples** | **Operations** | **Scale** |
|---------------|-------------|-----------------|--------------|----------------|-----------|
| **Quantitative** | Discrete | Countable whole numbers | Bank accounts, Children, Students | +, -, Ã—, Ã·, Statistics | Ratio |
| **Quantitative** | Continuous | Any numerical value | Weight, Height, Temperature* | +, -, Ã—, Ã·, Statistics | Interval/Ratio |
| **Qualitative** | Nominal | Categories, no order | Gender, Blood group, Colors | Count, Mode | Nominal |
| **Qualitative** | Ordinal | Categories with order | Ratings, Education level | Count, Mode, Median | Ordinal |

**Note:** Temperature in Celsius/Fahrenheit is Interval scale, while Kelvin is Ratio scale due to absolute zero.

## ğŸ“ Scales of Measurement
Understanding the scale of measurement is crucial for choosing appropriate statistical methods. There are four main scales:

### 1ï¸âƒ£ Nominal Scale Data
- **Definition:** Qualitative/Categorical data
- **Characteristics:**
  - Order does not matter
  - Categories with no ranking
  - Can only count frequencies and find mode
- **Examples:**
  - Favorite Color: Red (5) â†’ 50%, Blue (3) â†’ 30%, Orange (2) â†’ 20%
  - Gender: M, F
  - Blood Type: A, B, AB, O

### 2ï¸âƒ£ Ordinal Scale Data  
- **Definition:** Categorical data with meaningful order
- **Characteristics:**
  - Ranking is important
  - Order matters
  - Differences cannot be measured precisely
- **Examples:**
  - Rating Scale: 1 â†’ Best, 2 â†’ Good, 3 â†’ Bad
  - Education Level: High School < Bachelor's < Master's < PhD
  - Customer Satisfaction: Poor < Fair < Good < Excellent

### 3ï¸âƒ£ Interval Scale Data
- **Definition:** Numerical data with equal intervals
- **Characteristics:**
  - Order matters
  - Differences can be measured
  - **No true "0" starting point**
  - Ratios cannot be calculated meaningfully
- **Examples:**
  - Temperature: -30Â°F, -15Â°F, 30Â°F, 60Â°F, 90Â°F, 120Â°F
    - Difference: 60Â°F - 30Â°F = 30Â°F (meaningful)
    - Ratio: 90Â°F Ã· 30Â°F = 3:1 (NOT meaningful - 90Â°F is not "3 times hotter")

### 4ï¸âƒ£ Ratio Scale Data
- **Definition:** Numerical data with true zero point
- **Characteristics:**
  - Order matters âœ“
  - Differences are measurable âœ“
  - **Contains a true "0" starting point** âœ“
  - Ratios can be calculated meaningfully âœ“
- **Examples:**
  - Student marks in a class: 0, 90, 60, 30, 35, 40, 50
    - Mean = 30, 40, 50, 60, 35, 90
    - Differences: 40 - 30 = 10 points
    - Ratio: 90 Ã· 30 = 3:1 (90 is 3 times higher than 30)

## ğŸ“Š Scale Comparison Table

| **Scale** | **Order Matters** | **Measurable Differences** | **True Zero** | **Ratios Meaningful** | **Examples** |
|-----------|-------------------|----------------------------|---------------|---------------------|--------------|
| **Nominal** | âŒ | âŒ | âŒ | âŒ | Colors, Gender, Blood Type |
| **Ordinal** | âœ… | âŒ | âŒ | âŒ | Ratings, Education Level |
| **Interval** | âœ… | âœ… | âŒ | âŒ | Temperature (Â°F, Â°C) |
| **Ratio** | âœ… | âœ… | âœ… | âœ… | Height, Weight, Age, Income |

## ğŸ“ˆ Key Concepts

### Data
**Definition:** Facts or pieces of information that can be collected, measured, and analyzed.

**Example:** Heights of students in a classroom
```
{135 cm, 180 cm, 190 cm, 160 cm, 145 cm, 175 cm, 168 cm, 172 cm}
```

## ğŸ”„ The Statistical Process
```mermaid
flowchart TD
    A[ğŸ“Š Statistics] --> B[ğŸ“‹ Data Collection]
    B --> C[ğŸ—‚ï¸ Data Organization]
    C --> D[ğŸ” Data Analysis]
    D --> E[ğŸ“Š Interpretation]
    E --> F[ğŸ’¡ Decision Making]

    style A fill:#e1f5fe,color:#000
    style B fill:#f3e5f5,color:#000
    style C fill:#fff3e0,color:#000
    style D fill:#e8f5e8,color:#000
    style E fill:#fce4ec,color:#000
    style F fill:#f1f8e9,color:#000
```

## ğŸ“Š Data Visualization Examples

### Sample Dataset: Student Heights
```
Student A: 135 cm
Student B: 180 cm  
Student C: 190 cm
Student D: 160 cm
Student E: 145 cm
Student F: 175 cm
Student G: 168 cm
Student H: 172 cm
```

### Basic Statistics from Our Example:
- **Count**: 8 students
- **Range**: 135 cm - 190 cm (55 cm difference)
- **Mean**: ~165.6 cm
- **Median**: ~170 cm
- **Mode**: No mode (all values unique)

## ğŸ¯ Why Statistics Matters
Statistics helps us:
- âœ… Make data-driven decisions
- âœ… Identify patterns and trends  
- âœ… Predict future outcomes
- âœ… Test hypotheses
- âœ… Reduce uncertainty
- âœ… Choose appropriate analysis methods based on data scale

## ğŸ“š Common Statistical Applications

| Field | Application |
|-------|-------------|
| ğŸ¥ **Healthcare** | Clinical trials, disease tracking |
| ğŸ’° **Business** | Market research, sales forecasting |
| ğŸ« **Education** | Student performance analysis |
| ğŸŒ¡ï¸ **Weather** | Climate modeling, predictions |
| ğŸƒ **Sports** | Player statistics, performance metrics |

## ğŸ” Statistical Workflow
```mermaid
graph LR
    A[ğŸ¤” Question] --> B[ğŸ“Š Collect Data]
    B --> C[ğŸ§¹ Clean Data]
    C --> D[ğŸ“ˆ Analyze]
    D --> E[ğŸ“Š Visualize]
    E --> F[ğŸ’­ Interpret]
    F --> G[ğŸ“‹ Report]

    style A fill:#ffebee,color:#000
    style G fill:#e8f5e8,color:#000
```

## ğŸ“– Next Steps
1. **Learn about probability concepts**
2. **Explore data visualization techniques** (charts, graphs)
3. **Understand hypothesis testing**
4. **Practice with real datasets**

---

### ğŸ“ Quick Reference
```
ğŸ“Š Statistics = Science of Data
ğŸ“‹ Data = Facts/Information  
ğŸ“ˆ Analysis = Finding Patterns
ğŸ’¡ Goal = Better Decisions
ğŸŒ Population = Entire group of interest
ğŸ“‹ Sample = Subset of population
ğŸ” Descriptive = Organizing & summarizing data
ğŸ¯ Inferential = Making conclusions from data

ğŸ² Random Variable = Mapping random outcomes to numbers
ğŸª™ Examples: Coin toss (0,1), Dice roll (1-6), Temperature
ğŸ”¢ Discrete RV = Countable values (dice, coins)
ğŸ“ˆ Continuous RV = Any value in range (temperature, height)

ğŸ“š Set Theory Basics:
ğŸ”— A âˆ© B = Intersection (common elements)
ğŸ”„ A âˆª B = Union (all elements from both sets)
â– A - B = Difference (elements in A but not B)
âŠ† A âŠ† B = Subset (A is contained in B)
âŠ‡ B âŠ‡ A = Superset (B contains A)

ğŸ”¢ Quantitative = Numerical data (Discrete + Continuous)
ğŸ“ Qualitative = Categorical data (Nominal + Ordinal)
ğŸ¯ Discrete = Countable numbers (1, 2, 3...)
ğŸ“ˆ Continuous = Any value (1.5, 2.7, 3.14...)
ğŸ·ï¸ Nominal = No order (Gender, Colors)
ğŸ“Š Ordinal = Has order (Ratings, Grades)

ğŸ“ Scales of Measurement:
ğŸ·ï¸ Nominal = Categories, no order (Gender, Colors)
ğŸ“Š Ordinal = Categories with order (Ratings, Education)  
ğŸ“ Interval = Equal intervals, no true zero (Temperature Â°C/Â°F)
ğŸ“ Ratio = True zero point, ratios meaningful (Height, Weight, Age)

ğŸ“Š Central Tendency:
ğŸ“Š Mean = Arithmetic average (Î£x/n)
ğŸ“Š Median = Middle value when sorted
ğŸ“Š Mode = Most frequent value

ğŸ“ Measures of Dispersion:
ğŸ”¢ Variance = Average of squared differences from mean
   â€¢ Population: ÏƒÂ² = Î£(Xi-Î¼)Â²/N  
   â€¢ Sample: SÂ² = Î£(Xi-xÌ„)Â²/(n-1)
ğŸ“ Standard Deviation = Square root of variance
   â€¢ Population: Ïƒ = âˆšÏƒÂ²
   â€¢ Sample: S = âˆšSÂ²
ğŸ¯ Bessel's Correction = Use (n-1) for unbiased sample estimates

ğŸ“ˆ Normal Distribution (68-95-99.7 Rule):
â€¢ 68% of data within Â±1Ïƒ of mean
â€¢ 95% of data within Â±2Ïƒ of mean  
â€¢ 99.7% of data within Â±3Ïƒ of mean

ğŸ“Š Histograms and Distribution:
ğŸ“Š Histogram = Visual representation of data distribution
ğŸ“ Bin size affects appearance and interpretation
ğŸ”” Normal Distribution: Mean = Median = Mode
ğŸ“ˆ Right Skewed: Mean > Median > Mode (positive skew)
ğŸ“‰ Left Skewed: Mean < Median < Mode (negative skew)
ğŸ“¦ Box plots show quartiles, outliers, and skewness
ğŸ¯ Always visualize data before choosing statistical methods
```

---

*Remember: Good statistics start with good data! Always ensure your data is accurate, relevant, and properly collected.*
